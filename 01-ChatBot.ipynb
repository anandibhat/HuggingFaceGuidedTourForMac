{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ea5726-5002-4c77-95af-9209411926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed709ca-ade5-4332-94c8-c2d8e9b09eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2bc62299d3437faa10a0ff2ccfca86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6399d81056435182214e32e89efa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f075063ec54310902a0dbd9a4b9b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e196724e5d407c8b1fe635d5ec791f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2853a01e5f46c89324dfa3b8491fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e037ebd35fe04c39a02001b3d71fe2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warnings about padding_side that cannot be rectified with current software:\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model_names = [\"microsoft/DialoGPT-small\", \"microsoft/DialoGPT-medium\", \"microsoft/DialoGPT-large\"]\n",
    "use_model_index = 2  # Change 0: small model, 1: medium, 2: large model (requires most resources!)\n",
    "model_name = model_names[use_model_index]\n",
    "          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # , padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344741ae-f4b6-4e8c-a7a8-b3e4f360b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat function: received a user input and chat-history and returns the model's reply and chat-history:\n",
    "def reply(input_text, history=None):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True), chat_history_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dd5da-73ae-46f5-b4ec-c1abef2c6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current history: Hi!<|endoftext|>Hiya!<|endoftext|>\n",
      "D_GPT: Hiya!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  What's up?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current history: Hi!<|endoftext|>Hiya!<|endoftext|>What's up?<|endoftext|>Nothing much, you?<|endoftext|>\n",
      "D_GPT: Nothing much, you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Here neither, pretty boring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current history: Hi!<|endoftext|>Hiya!<|endoftext|>What's up?<|endoftext|>Nothing much, you?<|endoftext|>Here neither, pretty boring!<|endoftext|>That's good.<|endoftext|>\n",
      "D_GPT: That's good.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I don't like it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current history: Hi!<|endoftext|>Hiya!<|endoftext|>What's up?<|endoftext|>Nothing much, you?<|endoftext|>Here neither, pretty boring!<|endoftext|>That's good.<|endoftext|>I don't like it.<|endoftext|>I don't like it either.<|endoftext|>\n",
      "D_GPT: I don't like it either.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  So, shall we play a game?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current history: Hi!<|endoftext|>Hiya!<|endoftext|>What's up?<|endoftext|>Nothing much, you?<|endoftext|>Here neither, pretty boring!<|endoftext|>That's good.<|endoftext|>I don't like it.<|endoftext|>I don't like it either.<|endoftext|>So, shall we play a game?<|endoftext|>I don't know.<|endoftext|>\n",
      "D_GPT: I don't know.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Do you know chess?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current history: Hi!<|endoftext|>Hiya!<|endoftext|>What's up?<|endoftext|>Nothing much, you?<|endoftext|>Here neither, pretty boring!<|endoftext|>That's good.<|endoftext|>I don't like it.<|endoftext|>I don't like it either.<|endoftext|>So, shall we play a game?<|endoftext|>I don't know.<|endoftext|>Do you know chess?<|endoftext|>I don't know anything about it.<|endoftext|>\n",
      "D_GPT: I don't know anything about it.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Do you know math?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History cut from torch.Size([1, 90]) to torch.Size([1, 80])\n",
      "Current history: ?<|endoftext|>Nothing much, you?<|endoftext|>Here neither, pretty boring!<|endoftext|>That's good.<|endoftext|>I don't like it.<|endoftext|>I don't like it either.<|endoftext|>So, shall we play a game?<|endoftext|>I don't know.<|endoftext|>Do you know chess?<|endoftext|>I don't know anything about it.<|endoftext|>Do you know math?<|endoftext|>I don't know anything about it.<|endoftext|>\n",
      "D_GPT: I don't know anything about it.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  How much is 2 plus 2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History cut from torch.Size([1, 97]) to torch.Size([1, 80])\n",
      "Current history:  good.<|endoftext|>I don't like it.<|endoftext|>I don't like it either.<|endoftext|>So, shall we play a game?<|endoftext|>I don't know.<|endoftext|>Do you know chess?<|endoftext|>I don't know anything about it.<|endoftext|>Do you know math?<|endoftext|>I don't know anything about it.<|endoftext|>How much is 2 plus 2?<|endoftext|>I don't know anything about it.<|endoftext|>\n",
      "D_GPT: I don't know anything about it.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  How's the weather?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History cut from torch.Size([1, 91]) to torch.Size([1, 80])\n",
      "Current history:  don't like it either.<|endoftext|>So, shall we play a game?<|endoftext|>I don't know.<|endoftext|>Do you know chess?<|endoftext|>I don't know anything about it.<|endoftext|>Do you know math?<|endoftext|>I don't know anything about it.<|endoftext|>How much is 2 plus 2?<|endoftext|>I don't know anything about it.<|endoftext|>How's the weather?<|endoftext|>It's cloudy.<|endoftext|>\n",
      "D_GPT: It's cloudy.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Here it's sunny.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History cut from torch.Size([1, 95]) to torch.Size([1, 80])\n",
      "Current history: <|endoftext|>I don't know.<|endoftext|>Do you know chess?<|endoftext|>I don't know anything about it.<|endoftext|>Do you know math?<|endoftext|>I don't know anything about it.<|endoftext|>How much is 2 plus 2?<|endoftext|>I don't know anything about it.<|endoftext|>How's the weather?<|endoftext|>It's cloudy.<|endoftext|>Here it's sunny.<|endoftext|>I don't know anything about it.<|endoftext|>\n",
      "D_GPT: I don't know anything about it.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  What temperature do you have?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History cut from torch.Size([1, 96]) to torch.Size([1, 80])\n",
      "Current history:  know anything about it.<|endoftext|>Do you know math?<|endoftext|>I don't know anything about it.<|endoftext|>How much is 2 plus 2?<|endoftext|>I don't know anything about it.<|endoftext|>How's the weather?<|endoftext|>It's cloudy.<|endoftext|>Here it's sunny.<|endoftext|>I don't know anything about it.<|endoftext|>What temperature do you have?<|endoftext|>I don't know anything about it.<|endoftext|>\n",
      "D_GPT: I don't know anything about it.\n"
     ]
    }
   ],
   "source": [
    "history = None\n",
    "while True:\n",
    "    input_text = input(\"> \")\n",
    "    if input_text in [\"\", \"bye\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    reply_text, history_new = reply(input_text, history)\n",
    "    history=history_new\n",
    "    if history.shape[1]>80:\n",
    "        old_shape = history.shape\n",
    "        history = history[:,-80:]\n",
    "        print(f\"History cut from {old_shape} to {history.shape}\")\n",
    "    # history_text = tokenizer.decode(history[0])\n",
    "    # print(f\"Current history: {history_text}\")\n",
    "    print(f\"D_GPT: {reply_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd968e4-a722-45af-888a-79ba4a1e576f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
