{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ea5726-5002-4c77-95af-9209411926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed709ca-ade5-4332-94c8-c2d8e9b09eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7af4588f8844f4824ac2b1c8ec3f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warnings about padding_side that cannot be rectified with current software:\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model_names = [\"microsoft/DialoGPT-small\", \"microsoft/DialoGPT-medium\", \"microsoft/DialoGPT-large\"]\n",
    "use_model_index = 2  # Change 0: small model, 1: medium, 2: large model (requires most resources!)\n",
    "model_name = model_names[use_model_index]\n",
    "          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, framework='pt') # , padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344741ae-f4b6-4e8c-a7a8-b3e4f360b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat function: received a user input and chat-history and returns the model's reply and chat-history:\n",
    "def reply(input_text, history=None):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True), chat_history_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b0086-ea96-4ddf-872c-43418bf583a3",
   "metadata": {},
   "source": [
    "> **Note:** while executing a notebook-cell is done with `SHIFT-Enter`, only use `Enter` at the dialog input-prompt `> `, otherwise the notebook seems to hang...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dd5da-73ae-46f5-b4ec-c1abef2c6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter (not SHIFT-enter) after your input:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Good morning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_GPT: Good morning!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  How are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_GPT: I'm doing well, thank you for asking. How are you?\n"
     ]
    }
   ],
   "source": [
    "history = None\n",
    "first = True\n",
    "while True:\n",
    "    if first is True:\n",
    "        first = False\n",
    "        print(\"Please press enter (not SHIFT-enter) after your input:\")\n",
    "    input_text = input(\"> \")\n",
    "    if input_text in [\"\", \"bye\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    reply_text, history_new = reply(input_text, history)\n",
    "    history=history_new\n",
    "    if history.shape[1]>80:\n",
    "        old_shape = history.shape\n",
    "        history = history[:,-80:]\n",
    "        print(f\"History cut from {old_shape} to {history.shape}\")\n",
    "    # history_text = tokenizer.decode(history[0])\n",
    "    # print(f\"Current history: {history_text}\")\n",
    "    print(f\"D_GPT: {reply_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd968e4-a722-45af-888a-79ba4a1e576f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
